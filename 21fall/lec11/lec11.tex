\documentclass{beamer}
\usepackage{tikz,amsmath,hyperref,graphicx,stackrel}
\usetikzlibrary{positioning,shadows,arrows,shapes,calc}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\mode<presentation>{\usetheme{Frankfurt}}
\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}
\title{Lecture 11: Gaussian and Gaussian  Mixture Classifiers}
\author{Mark Hasegawa-Johnson}
\date{ECE 417: Multimedia Signal Processing, Fall 2021}  
\begin{document}

% Title
\begin{frame}
  \maketitle
\end{frame}

% Title
\begin{frame}
  \tableofcontents
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[Classifiers]{Classifiers}
\setcounter{subsection}{1}

\begin{frame}
  \frametitle{What is a Classifier?}

  A {\bf classifier} is a function $f:{\mathcal X}\rightarrow{\mathcal
    Y}$ that takes as input feature $x\in{\mathcal X}$, and generates
  an output classification label $y\in{\mathcal Y}$.

\end{frame}

\begin{frame}
  \frametitle{Example: A Dog/Cat Classifier}

  \centerline{\includegraphics[height=0.8\textheight]{exp/1024px-Perceptron_example.svg.png}}
  {\tiny CC-BY 4.0, Elizabeth Goodspeed, 2015}
\end{frame}

\begin{frame}
  \frametitle{Defining the Classifier}

  \begin{itemize}
  \item
    Often, $\vec{x}$ is a real-valued feature vector of some kind,
    $\vec{x}\in\Re^d$, and $y$ is a class label (a word specifying the type of
    object).
  \item
    Then we  say that $y=f(\vec{x})$ is the label generated by the classifier.
  \item
    The task of classifier design is the task of designing the function $f(\cdot)$.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[Bayesian]{Bayesian Classifiers}
\setcounter{subsection}{1}

\begin{frame}
  \frametitle{Minimum Probability of Error}

  Suppose we want to minimize the probability of an error.  In order
  to analyze this problem, define three random variables like this:
  \begin{itemize}
  \item
    $\vec{X}$ is the observation vector, and $\vec{x}$ is its instance value.
  \item
    $Y$ is the (unknown) true class label, and $y$ is its instance value.
  \item
    $\hat{Y}=f(\vec{X})$ is the classifier's output hypothesis, and $\hat{y}=f(\vec{x})$ is
    its instance value.  It might be right ($\hat{y}=y$)  or it might be wrong
    ($\hat{y}\ne y$).
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Minimum Probability of Error}

  Suppose we know $\vec{x}$, but we don't know $y$.  The classifier
  generates $\hat{y}$.  The probability of an error is
  \begin{align*}
    \Pr\{E\} &= \Pr\left\{Y \ne \hat{y}\right\}
  \end{align*}
  For any particular $\vec{x}$, we want to choose the value of $\hat{y}$
  that has the lowest probability of error:
  \begin{align*}
    f(\vec{x}) &= \argmin_{\hat{y}} \Pr\left\{Y \ne \hat{y}|\vec{X}=\vec{x}\right\}\\
    &= \argmax_{\hat{y}} \Pr\left\{Y = \hat{y}|\vec{X}=\vec{x}\right\}\\
    &= \argmax_{\hat{y}} p_{Y|\vec{X}}(\hat{y}|\vec{x})
  \end{align*}
  where the last line introduces a pmf notation:
  \[
  p_{Y|\vec{X}}(y|\vec{x}) = \Pr\left\{Y=y|\vec{X}=\vec{x}\right\}
  \]
\end{frame}
    
\begin{frame}
  \frametitle{The Bayesian Classifier}

  Unfortunately, $p_{Y|\vec{X}}(y|\vec{x})$ is often very hard to
  calculate directly.  Instead, it is often easier to calculate it
  using Bayes' rule:
  \begin{align*}
    f(\vec{x}) &= \argmax_y p_{Y|\vec{X}}(y|\vec{x})\\
    &= \argmax_y \frac{p_{\vec{X}|Y}(\vec{x}|y)p_Y(y)}{p_{\vec{X}}(\vec{x})}
  \end{align*}
  It is often much easier to learn $p_{\vec{X}|Y}(\vec{x}|y)$ from data, instead of
  $p_{Y|\vec{X}}(y|\vec{x})$.  Let's discuss why.
\end{frame}

\begin{frame}
  \frametitle{The Four Bayesian Probabilities}

  The four probabilities in Bayes' rule are:
  \begin{itemize}
  \item
    The {\bf prior} $p_Y(y)$ is the probability that $Y=y$ before you
    see any observations.
  \item
    The {\bf posterior} $p_{Y|\vec{X}}(y|\vec{x})$ is the probability
    that $Y=y$ after you see an observation.
  \item
    The {\bf likelihood} $p_{\vec{X}|Y}(\vec{x}|y)$ is the probability
    density that the class $Y=y$ might ``generate'' the observation
    $\vec{X}=\vec{x}$.  
  \item
    The {\bf evidence} $p_{\vec{X}}(\vec{x})$ is the probability
    density that the observation $\vec{x}$ gets generated by any
    natural process, regardless of the class.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The Four Bayesian Probabilities}

  The prior and the likelihood are usually very easy to estimate from
  training data.  The posterior and the evidence are usually much
  harder to estimate.  But fortunately, using Bayes' rule, we don't
  need to estimate the evidence!  That's because:
  \begin{align*}
    f(\vec{x}) &= \argmax_y p_{Y|\vec{X}}(y|\vec{x})\\
    &= \argmax_y \frac{p_{\vec{X}|Y}(\vec{x}|y)p_Y(y)}{p_{\vec{X}}(\vec{x})}
    &= \argmax_y p_{\vec{X}|Y}(\vec{x}|y)p_Y(y)
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Example: Fisher's Iris Data}

  The rest of this lecture will use example data from Ronald Fisher's
  paper ``The use of multiple measurements in taxonomic problems,'' in
  which he classified three different species of irises: Iris setosa,
  Iris versicolor, and Iris virginica.

  \centerline{\includegraphics[width=0.25\textwidth]{exp/450px-Kosaciec_szczecinkowaty_Iris_setosa.jpg}
    \includegraphics[width=0.25\textwidth]{exp/800px-Iris_versicolor_3.jpg}
    \includegraphics[width=0.25\textwidth]{exp/736px-Iris_virginica.jpg}}

  {\tiny CC-SA 3.0, Radomil, 2005; CC-SA 3.0, Dianglois, 2005; CC-SA 4.0, Eric Hunt, 2018}
\end{frame}

\begin{frame}
  \frametitle{Example: Fisher's Iris Data: Likelihood}

  Fisher made four measurements from each flower:
  \centerline{\includegraphics[height=0.9\textheight]{exp/600px-Iris_dataset_scatterplot.svg.png}}
  {\tiny CC-BY 4.0, Nicoguaro, 2016}
    
\end{frame}

\begin{frame}
  \frametitle{Example: Fisher's Iris Data: Likelihood}

  The rest of this lecture will focus on just the first two
  measurements from each flower: sepal length, and sepal width.
  \centerline{\includegraphics[height=0.8\textheight]{exp/scatter.png}}
    
\end{frame}

\begin{frame}
  \frametitle{Example: Fisher's Iris Data: Prior}

  For convenience, we can, say, 50 examples from each of the three
  species, so that our prior probabilities are
  \[
  p_Y(0) = p_Y(1) = p_Y(2) = \frac{1}{3}
  \]
  \centerline{\includegraphics[height=0.6\textheight]{exp/classlabels.png}}
\end{frame}

\begin{frame}
  \frametitle{Example: Fisher's Iris Data: Prior}

  {\bf Recap:} in order to classify these flowers, we need to know the
  prior, and the likelihood.  By design, we know the prior:
  \[
  p_Y(0) = p_Y(1) = p_Y(2) = \frac{1}{3}
  \]
  We can estimate the likelihood based on the scatterplots.  But {\bf
    how} can we estimate the likelihood from the scatterplots?
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[Gaussian]{Gaussian Classifiers}
\setcounter{subsection}{1}

\begin{frame}
  \frametitle{Gaussian Likelihoods}

  Notice that, in Fisher's feature space, the scatter plot for each
  class looks kind of  elliptical.  Gaussians have an elliptical shape.
  So let's use a Gaussian:
  \begin{align*}
    p_{\vec{X}|Y}(\vec{x}|y) &= {\mathcal N}(\vec{x}|\vec\mu_y,\Sigma_y)\\
    &= \frac{1}{(2\pi)^{D/2}|\Sigma_y|^{1/2}}
    e^{-\frac{1}{2}(\vec{x}-\vec\mu_y)^T\Sigma_y^{-1}(\vec{x}-\vec\mu_y)}
  \end{align*}
  where $D$ is the vector dimension ($D=2$, in our case), and
  \begin{itemize}
  \item $\vec\mu_y$ is the mean of class $y$,
  \item $\Sigma_y$ is the covariance matrix of class $y$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Gaussian Likelihoods}

  Here are the contour plots of the Gaussian likelihood models that best fit
  each of the three data clouds:
  \centerline{\includegraphics[height=0.8\textheight]{exp/gaussian_contours.png}}
\end{frame}

\begin{frame}
  \frametitle{Gaussian Classifiers}

  If the likelihoods are Gaussian, then the classification rule for
  three classes ($y\in\{0,1,2\}$) becomes:
  \begin{displaymath}
    f(\vec{x}) =\argmax_y p_Y(y){\mathcal N}(\vec{x}|\vec\mu_y,\Sigma_y)
  \end{displaymath}
\end{frame}

\begin{frame}
  \frametitle{Gaussian Classifiers}

  Here are the classification regions.  Any flower with a feature
  vector in the blue region is Iris sentosa, any in the green region
  is Iris versicolor, any in the blue region is Iris virginica.
  \centerline{\includegraphics[height=0.8\textheight]{exp/gaussian_classifier.png}}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[Parameters]{Maximum Likelihood Parameter Estimation}
\setcounter{subsection}{1}

\begin{frame}
  \frametitle{The parameter estimation problem}

  \begin{itemize}
  \item
    In the previous slides, I showed you the ``optimum'' Gaussians for
    this data.
  \item 
    Those included some ``optimum'' values of the parameters
    $\vec\mu_0,\vec\mu_1,\vec\mu_2,\Sigma_0,\Sigma_1,\Sigma_2$.
  \item
    How did I find those values?  ``Optimum'' in what  sense?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Maximum likelihood parameter estimation}

  I estimated those using a method called {\bf maximum likelihood}
  parameter estimation.  Maximum likelihood (ML) parameter estimation
  finds the parameter set,
  $\Theta=\{\vec\mu_0,\vec\mu_1,\vec\mu_2,\Sigma_0,\Sigma_1,\Sigma_2\}$,
  according to the following rule:
  \[
  \Theta = \argmax \sum_{i=1}^n \ln p(\vec{x}_i|y_i;\Theta)
  \]
  in other words, we just choose the parameters that best explain the
  training data.

  The {\bf training dataset}, ${\mathcal
    D}=\{(\vec{x}_1,y_1),\ldots,(\vec{x}_n,y_n)\}$, is a set of
  labeled examples: for the $i^{\textrm{th}}$ flower, I know both its
  feature vector $\vec{x}_i$, and its true class label $y_i$.
\end{frame}

\begin{frame}
  \frametitle{ML Parameter Estimates for a Gaussian}

  Gaussian likelihood functions are very nice because the
  log-likelihood is a quadratic form:
  \begin{align*}
    {\mathcal L} &=\sum_{i=1}^n \ln p(\vec{x}_i|y_i;\Theta)\\
    &=-\frac{1}{2}\sum_{i=1}^n \left(D\ln(2\pi)+\ln|\Sigma_{y_i}|
    + (\vec{x}_i-\vec\mu_{y_i})^T\Sigma_{y_i}^{-1}(\vec{x}_i-\vec\mu_{y_i})\right)
  \end{align*}
  You already know how to differentiate that:
  \begin{displaymath}
    \nabla_{\vec\mu_y}{\mathcal L} =
    \sum_{i:y_i=y}\Sigma_{y}^{-1}(\vec{x}_i-\vec\mu_{y})
  \end{displaymath}
  Left as an exercise for the reader: set that equation to 0, and
  solve for the optimum value of $\vec\mu_y$.
\end{frame}

\begin{frame}
  \frametitle{ML Parameter Estimates for a Gaussian}

  The ML estimates of the parameters for a Gaussian therefore wind up
  with an incredibly simple form.
  \begin{align*}
    \vec\mu_y &= \frac{\sum_{i=1}^n \delta_i(y) \vec{x}_i}{\sum_{i=1}^n\delta_i(y)}\\
    \Sigma_y &= \frac{\sum_{i=1}^n \delta_i(y) (\vec{x}_i-\vec\mu_y)(\vec{x}_i-\vec\mu_y)^T}{\sum_{i=1}^n\delta_i(y)}
  \end{align*}
  where $\delta_i(y)$ is just an indicator function, counting up the
  number of examples in each class:
  \begin{displaymath}
    \delta_i(y)=\begin{cases}
    1 & y_i=y\\
    0 & \mbox{otherwise}
    \end{cases}
  \end{displaymath}
\end{frame}

\begin{frame}
  \frametitle{ML Estimates of the Gaussian Mean}

  Each mean vector is just the average of the training data for its class:
  \centerline{\includegraphics[height=0.8\textheight]{exp/means.png}}
\end{frame}

\begin{frame}
  \frametitle{ML Estimates of the Gaussian Covariance}

  Each covariance matrix is just the average quadratic spread of data
  points around the mean.  Here I'm plotting the eigenvectors of each
  covariance matrix, scaled by the square root of their corresponding
  eigenvalues:
  
  \centerline{\includegraphics[height=0.8\textheight]{exp/covariances.png}}
\end{frame}

\begin{frame}
  \frametitle{ML Estimates of the Gaussian Parameters}

  \ldots and again, that gives us these Gaussian  likelihood functions:
  
  \centerline{\includegraphics[height=0.8\textheight]{exp/gaussian_contours.png}}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[GMM]{Gaussian Mixture Models}
\setcounter{subsection}{1}

\begin{frame}
  \frametitle{What if a Gaussian is not a good model?}

  Sometimes, a Gaussian is not a good model of the data.  For example,
  in the iris dataset, classes 0 and 1 are well fit by Gaussians, but
  class 2 doesn't really fit very well:
  \centerline{\includegraphics[height=0.8\textheight]{exp/gaussian2_lackoffit.png}}
\end{frame}

\begin{frame}
  \frametitle{Can we have more than one Gaussian per class?}

  One way to improve the fit is by having more than one Gaussian per class:
  \centerline{\includegraphics[height=0.8\textheight]{exp/em_class2.png}}
\end{frame}

\begin{frame}
  \frametitle{Gaussian Mixture Model}

  This is called a {\bf Gaussian mixture model} of the likelihood
  (GMM).  It has the following form:
  \begin{displaymath}
    p_{\vec{X}|Y}(\vec{x}|y)
    = \sum_{k=0}^{K-1} c_{y,k} {\mathcal N}(\vec{x}|\vec\mu_{y,k},\Sigma_{y,k})
  \end{displaymath}
  where
  \begin{displaymath}
    c_{y,k}>0,~~\sum_{k=1}^K c_{y,k}=1
  \end{displaymath}
\end{frame}

\begin{frame}
  \frametitle{Advantages and Disadvantages of GMM vs. Gaussian}

  \begin{description}
  \item[Disadvantage:]~ The GMM has more parameters to train.  For
    example, if there are three classes, and $K=2$ Gaussians per
    class, then you have to learn $3\times 2=6$ mean vectors, and
    $3\times 2=6$ covariance matrices.  
  \item[Advantage:]~ The GMM is more flexible.  For example, a
    Gaussian can only learn a pdf with one cluster, but a GMM can learn a
    pdf with $K$ clusters.
  \item[Terminology:]~Each of the Gaussians is called a {\bf cluster},
    the mean vector $\vec\mu_{y,k}$ is called the {\bf cluster
      centroid} or {\bf cluster mean}, and the covariance matrix
    $\Sigma_{y,k}$ is called the {\bf cluster covariance}.
  \end{description}
\end{frame}


\begin{frame}
  \frametitle{Another GMM Example}

  Here's an example of a 1D Gaussian mixture model with $K=5$ clusters,
  from Wikipedia:
  \url{https://commons.wikimedia.org/wiki/File:Movie.gif}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[EM]{Parameter estimation using the Expectation  Maximization algorithm}
\setcounter{subsection}{1}

\begin{frame}
  \frametitle{Estimating parameters for a GMM}

  Now we have a much larger parameter set:
  \begin{displaymath}
    \Theta =\left\{c_{y,k},\vec\mu_{y,k},\Sigma_{y,k}:y\in\{0,1,2\},k\in\{0,1\}\right\}
  \end{displaymath}
  How do we estimate those parameters from training data?

  There are many ways it can be done.  I'm going to teach you the best
  one, which is called the {\bf expectation maximization} algorithm
  (EM).
\end{frame}

\begin{frame}
  \frametitle{Estimating parameters for a GMM}
  
  Surprise!  The answer is exactly the same as for the Gaussian:
  \begin{align*}
    \vec\mu_{y,k} &=
    \frac{\sum_{i=1}^n \gamma_i(y,k) \vec{x}_i}{\sum_{i=1}^n\gamma_i(y,k)}\\
    \Sigma_{y,k} &= \frac{\sum_{i=1}^n \gamma_i(y,k) (\vec{x}_i-\vec\mu_y)(\vec{x}_i-\vec\mu_y)^T}{\sum_{i=1}^n\gamma_i(y,k)}\\
    c_{y,k} &= \frac{\sum_{i=1}^n \gamma_i(y,k)}{\sum_{k=0}^{K-1}\sum_{i=1}^N\gamma_i(y,k)}
  \end{align*}
  When learning the parameters for a Gaussian, we had a counting variable,
  $\delta_i(y)$, which was $1$ if the $i^{\textrm{th}}$ token was from class $y$, and 0 otherwise.
  When learning a GMM, we have a probability instead, $0\le\gamma_i(y,k)\le 1$:
  \begin{displaymath}
    \gamma_i(y,k)=\Pr\left\{\vec{x}_i~\mbox{came from the}~k^{\textrm{th}}~\mbox{cluster in class}~y\right\}
  \end{displaymath}
\end{frame}

\begin{frame}
  \frametitle{The cluster posterior}

  \begin{displaymath}
    \gamma_i(y,k)
    =\Pr\left\{\vec{x}_i~\mbox{came from the}~k^{\textrm{th}}~\mbox{Gaussian of class}~y\right\}
  \end{displaymath}
  \begin{displaymath}
    =\delta_i(y)\Pr\left\{\mbox{cluster}=k|\vec{x}_i,y\right\}
  \end{displaymath}
  \begin{displaymath}
    =\delta_i(y)\frac{\Pr\left\{\mbox{cluster}=k,\vec{x}_i|y\right\}}{\Pr\left\{\vec{x}_i|y\right\}}
  \end{displaymath}
  \begin{displaymath}
    =\delta_i(y)\frac{c_{y,k} {\mathcal N}(\vec{x}_i|\vec\mu_{y,k},\Sigma_{y,k})}{\sum_{\ell=0}^{K-1}c_{y,\ell} {\mathcal N}(\vec{x}_i|\vec\mu_{y,\ell},\Sigma_{y,\ell})}
  \end{displaymath}
\end{frame}

\begin{frame}
  \frametitle{The cluster posterior}

  Wait a minute\ldots
  \begin{itemize}
  \item
    In order to estimate the model parameters $\vec\mu_{y,k}$,
    $\Sigma_{y,k}$, and $c_{y,k}$, we need to know the cluster
    posteriors $\gamma_i(y,k)$.
  \item
    But in order to estimate the cluster posteriors, we need to know
    $\vec\mu_{y,k}$, $\Sigma_{y,k}$, and $c_{y,k}$!
  \item
    This is sometimes called a \href{https://en.wikipedia.org/wiki/Chicken_or_the_egg}{\color{blue} chicken-and-egg problem}.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Initialization: Solving the Chicken-and-egg problem}

  \begin{itemize}
    \item 
      The way that we solve the chicken-and-egg problem is by
      ``guessing'' some initial values of $\vec\mu_{y,k}$,
      $\Sigma_{y,k}$, and $c_{y,k}$.  Then, given our initial guess,
      we calculate $\gamma_i(y,k)$, and that allows us to {\bf
        re-estimate} better values of $\vec\mu_{y,k}$,
      $\Sigma_{y,k}$, and $c_{y,k}$.
    \item
      Our initial guesses can be smart, or they can be stupid.  Stupid
      guesses (totally random) will lead to OK solutions; smart
      guesses may lead to better solutions.
    \item
      One smart guess is to start with the global Gaussian means, and
      add some small random vector $\vec\epsilon$:
      \begin{align*}
        \vec\mu_{y,0} &= \vec\mu_y+\vec\epsilon\\
        \vec\mu_{y,1} &= \vec\mu_y-\vec\epsilon
      \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Initialization: Solving the Chicken-and-egg problem}

  Here are some initial guesses for the Iris dataset:
  \centerline{\includegraphics[height=0.8\textheight]{exp/gmm_initialmeans.png}}
\end{frame}
  
\begin{frame}
  \frametitle{Iterate Until Convergence}

  Then we iterate the following until convergence:
  \begin{enumerate}
  \item
    Using $\vec\mu_{y,k}$, $\Sigma_{y,k}$, and $c_{y,k}$, calculate
    $\gamma_i(y,k)$.
  \item
    Using $\gamma_i(y,k)$, re-estimate $\vec\mu_{y,k}$, $\Sigma_{y,k}$, and $c_{y,k}$.
  \item
    If $\vec\mu_{y,k}$ changed more than about 5\%, then go back to
    step 1.  Otherwise, you're done.
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Iterate Until Convergence}

  Here are what the mean vectors look like after the EM algorithm converges:
  \centerline{\includegraphics[height=0.8\textheight]{exp/em_convergedmeans.png}}
\end{frame}
  
\begin{frame}
  \frametitle{Estimating parameters for a GMM}
  
  Putting it all together:
  \begin{enumerate}
  \item
    Using $\vec\mu_{y,k}$ and $\Sigma_{y,k}$, calculate $\gamma_i(y,k)$.
    \begin{align*}
      \gamma_i(y,k)
      &=\delta_i(y)\frac{c_{y,k} {\mathcal N}(\vec{x}_i|\vec\mu_{y,k},\Sigma_{y,k})}{\sum_{\ell=0}^{K-1}c_{y,\ell} {\mathcal N}(\vec{x}_i|\vec\mu_{y,\ell},\Sigma_{y,\ell})}
    \end{align*}
  \item
    Using $\gamma_i(y,k)$, re-estimate $\vec\mu_{y,k}$ and $\Sigma_{y,k}$.
    \begin{align*}
      \vec\mu_{y,k} &=
      \frac{\sum_{i=1}^n \gamma_i(y,k) \vec{x}_i}{\sum_{i=1}^n\gamma_i(y,k)}\\
      \Sigma_{y,k} &=
      \frac{\sum_{i=1}^n \gamma_i(y,k) (\vec{x}_i-\vec\mu_y)(\vec{x}_i-\vec\mu_y)^T}{\sum_{i=1}^n\gamma_i(y,k)}\\
    c_{y,k} &= \frac{\sum_{i=1}^n \gamma_i(y,k)}{\sum_{k=0}^{K-1}\sum_{i=1}^N\gamma_i(y,k)}      
    \end{align*}
  \item
    If $\vec\mu_{y,k}$ changed more than about 5\%, then go back to
    step 1.  Otherwise, you're done.
  \end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[GMC]{Gaussian Mixture Classifiers}
\setcounter{subsection}{1}

\begin{frame}
  \frametitle{Gaussian Mixture Classifier}

  A Gaussian mixture classifier is a Bayesian classifier with GMM likelihoods:
  \begin{displaymath}
    f(\vec{x}) = \argmax_y p_Y(y)p_{\vec{X}|Y}(\vec{x}|y)
  \end{displaymath}
\end{frame}

\begin{frame}
  \frametitle{Gaussian Mixture Classifier}
  Here's what that  looks like for the Iris data:
  \centerline{\includegraphics[height=0.8\textheight]{exp/em_classifier.png}}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[Summary]{Summary}
\setcounter{subsection}{1}

\begin{frame}
  \frametitle{Bayesian Classifier}

  \begin{align*}
    f(\vec{x}) 
    &= \argmax_y p_{\vec{X}|Y}(\vec{x}|y)p_Y(y)
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{ML Parameter Estimates for a Gaussian}

  \begin{align*}
    \vec\mu_y &= \frac{\sum_{i=1}^n \delta_i(y) \vec{x}_i}{\sum_{i=1}^n\delta_i(y)}\\
    \Sigma_y &= \frac{\sum_{i=1}^n \delta_i(y) (\vec{x}_i-\vec\mu_y)(\vec{x}_i-\vec\mu_y)^T}{\sum_{i=1}^n\delta_i(y)}
  \end{align*}
  where $\delta_i(y)$ is just an indicator function, counting up the
  number of examples in each class:
  \begin{displaymath}
    \delta_i(y)=\begin{cases}
    1 & y_i=y\\
    0 & \mbox{otherwise}
    \end{cases}
  \end{displaymath}
\end{frame}

\begin{frame}
  \frametitle{Estimating parameters for a GMM}
  
  Putting it all together:
  \begin{enumerate}
  \item
    Using $\vec\mu_{y,k}$ and $\Sigma_{y,k}$, calculate $\gamma_i(y,k)$.
    \begin{align*}
      \gamma_i(y,k)
      &=\delta_i(y)\frac{c_{y,k} {\mathcal N}(\vec{x}_i|\vec\mu_{y,k},\Sigma_{y,k})}{\sum_{\ell=0}^{K-1}c_{y,\ell} {\mathcal N}(\vec{x}_i|\vec\mu_{y,\ell},\Sigma_{y,\ell})}
    \end{align*}
  \item
    Using $\gamma_i(y,k)$, re-estimate $\vec\mu_{y,k}$ and $\Sigma_{y,k}$.
    \begin{align*}
      \vec\mu_{y,k} &=
      \frac{\sum_{i=1}^n \gamma_i(y,k) \vec{x}_i}{\sum_{i=1}^n\gamma_i(y,k)}\\
      \Sigma_{y,k} &=
      \frac{\sum_{i=1}^n \gamma_i(y,k) (\vec{x}_i-\vec\mu_y)(\vec{x}_i-\vec\mu_y)^T}{\sum_{i=1}^n\gamma_i(y,k)}\\
      c_{y,k} &= \frac{\sum_{i=1}^n \gamma_i(y,k)}{\sum_{k=0}^{K-1}\sum_{i=1}^N\gamma_i(y,k)}      
    \end{align*}
  \item
    If $\vec\mu_{y,k}$ changed more than about 5\%, then go back to
    step 1.  Otherwise, you're done.
  \end{enumerate}
\end{frame}

\end{document}

