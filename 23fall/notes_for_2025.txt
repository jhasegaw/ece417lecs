Lectures 1 and 2: Don't teach eigenvectors and SVD.  Instead,
basically, teach what is currently the content of HW1, and make the HW
a set of sample problems instead --- use the current lec07 in place of
lec01?  Include: (1) derivative of scalar w.r.t. vector (gradient) and
w.r.t. matrix (which has dimensions equal to transpose of the matrix),
(2) orthogonal projection onto a vector, onto an orthogonal basis, and
onto a non-orthogonal frame, (3) pseudo-inverse.  Include the formal
definition A=AA^\dagA, A^\dag=A^\dagAA^\dag, and the relationship to
orthogonal projection, and the algebraic forms for the short-fat and
tall-thin matrices.

Use notation like the wikipedia page for "matrix calculus:" bf for
vectors, capital bf for matrices.  Boldface is better than the vector
superscript because I can recreate it in HTML.

Lecture 6: can I figure out why Griffin-Lim doesn't converge to
exactly the desired signal?  Maybe underconstrained b/c no overlap in
first and last half frames -- maybe need to have a signal with zeros
in the first and last half frame?

HMMs: Can I use capital letters for the random variables, e.g.,

a_{i,j} = Pr(Q_{t+1}=j|Q_t=i)
b_j(x_t) = Pr(X_t=x_t|Q_t=j)
alpha_t(i) = Pr(X_1=x_1,...,X_t=x_t,Q_t=i)
beta_t(i) = Pr(X_{t+1}=x_{t+1},...,X_T=x_T|Q_t=i)

