Maybe start with Bayesian classifiers -- maybe even mp4 first, then
mp3 using eigenface.  Then part 2 is vector space signal processing:
mp2=Griffin-Lim, mp1=barycentric coordinates.  Then part 3 is deep
learning: LSTM, autoencoder.

The vector derivative stuff has thoroughly confused them.  Many of
them still don't even seem to know what the norm symbol means!!
Instead, teach them how to turn every matrix equation into a set of
scalar summations, differentiate in scalar form, and then turn it back
into a matrix equation.



NO: Lectures 1 and 2: Don't teach eigenvectors and SVD.  Instead,
basically, teach what is currently the content of HW1, and make the HW
a set of sample problems instead --- use the current lec07 in place of
lec01?  Include: (1) derivative of scalar w.r.t. vector (gradient) and
w.r.t. matrix (which has dimensions equal to transpose of the matrix),
(2) orthogonal projection onto a vector, onto an orthogonal basis, and
onto a non-orthogonal frame, (3) pseudo-inverse.  Include the formal
definition A=AA^\dagA, A^\dag=A^\dagAA^\dag, and the relationship to
orthogonal projection, and the algebraic forms for the short-fat and
tall-thin matrices.

Use notation like the wikipedia page for "matrix calculus:" bf for
vectors, capital bf for matrices.  Boldface is better than the vector
superscript because I can recreate it in HTML.

Lecture 6: can I figure out why Griffin-Lim doesn't converge to
exactly the desired signal?  Maybe underconstrained b/c no overlap in
first and last half frames -- maybe need to have a signal with zeros
in the first and last half frame?

HMMs: Can I use capital letters for the random variables, e.g.,

a_{i,j} = Pr(Q_{t+1}=j|Q_t=i)
b_j(x_t) = Pr(X_t=x_t|Q_t=j)
alpha_t(i) = Pr(X_1=x_1,...,X_t=x_t,Q_t=i)
beta_t(i) = Pr(X_{t+1}=x_{t+1},...,X_T=x_T|Q_t=i)

